# Big-Data-Analytics
This repository contains a project which has as main goal to make an instruction to the basic steps of the process followed for applying data mining techniques, namely: collection, preprocessing / cleaning,
conversion, application of data mining techniques and evaluation. This project is done the Python programming language using the SciKit Learn tool and consists of the following tasks:

 1)The first task is to build a machine learning model that is capable of classifying the category of an article using text (**https://github.com/DimOriCoding/Big-Data-Analytics/blob/main/wordclouds.py**). The K-Nearest Neighbour (K-NN) approach (using also the Jaccard coefficient) is used in order to classify texts to one of following article categories: Business, Health, Entertainment, Technology. Additionally a word cloud for each in order to provide a general description of each one of the aforementioned categories is created(**https://github.com/DimOriCoding/Big-Data-Analytics/blob/main/wordclouds.py**). For creating a word cloud you will use all the articles in each one of the above categories (**https://drive.google.com/drive/folders/1FoV20irNflfpGscZMaJ6yzvhArwzNLmy**).

 
 2)The second task is to create a classifier for a dataset (train,test) of movies(**https://drive.google.com/drive/folders/1IRc669sTXSA0rBhzqXGfAjUnwomdh1dW**) by using K-Nearest Neighbour (K-NN) approach with Locality Sensitive Hashing (LSH). Firstly Brute-force approach is used, where each document in the test-set is compared to each document in the train-set. But this approach have quadratic time complexity, making them infeasible for big data scenarios. The goal is to speed up K-NN approach by devising a method that can quickly identify similar items without having to exhaustively compare each pair. So, Locality Sensitive Hashing (LSH) is a technique that efficiently approximates similarity search by reducing the dimensionality of data while preserving local distances between points. The core idea behind LSH is to hash data points(for example documents on train or test dataset for our task) in such a way that similar items are mapped to the same hash buckets with high probability. By doing so, we can drastically reduce the search space and expedite the process of finding similar items (**https://medium.com/@sarthakjoshi_9398/understanding-locality-sensitive-hashing-lsh-a-powerful-technique-for-similarity-search-a95b090bdc4a**). So in the LSH case  he actual similarity between two documents if the expected similarity is above a specific threshold is computed.

 3)The final task of this project is about Dynamic Time Warping (DTW), which is an algorithm used to compare two time-based datasets (like two sequences of numbers) to find similarities. It does this by adjusting the timings of the data points to minimize the difference between the two datasets. For a dataset including  two time series (**https://github.com/DimOriCoding/Big-Data-Analytics/blob/main/DTW.distances.csv**)The  DTW is used with euclidean distance to calculate the distance between them.

The above project is implemented in collaboration with Sofia Kalogiannidi which is a postgraduate student of the graduate (MSc) program  of the department of Informatics and Telecommunications of UoA named "Computer Science".
 

